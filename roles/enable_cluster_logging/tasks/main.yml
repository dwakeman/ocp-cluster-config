#########################################################################################
#
# This playbook is specific for a log forwarding only scneario where the internal
# ElasticSearch and Kibana components are not needed.  It does not install the
# ElasticSearch Operator.  Steps are based on OpenShift documentation.
#
# https://docs.openshift.com/container-platform/4.3/logging/cluster-logging-deploying.html
# https://docs.openshift.com/container-platform/4.3/logging/config/cluster-logging-external.html
#
#  Actions:
#    - Create the openshift-logging namespace for the Cluster Logging operator
#    - Install the Cluster Logging operator by creating a Subscription
#    - Create the ClusterLogging custom resource
#    - Create the LogForwarding custom resource
#
#  Variables:
#    log_forwarding_host - the URL of the external server where logs will be forwarded
#    log_forwarding_port - the port on which the server is listening for FluentD traffic  
#
# Note: Testing showed that ElasticSearch would not run if the nodes had less than 32GB
#       RAM.  The pods would fail with a vm.max_map_count too low error.  This playbook
#       does not configure ElasticSearch, but if you update the playbook to include it 
#       be sure that your nodes have at least 32GB of RAM.
#
#########################################################################################

- name: Create openshift-logging namespace
  k8s:
    state: present
    definition: "{{ lookup('file', 'logging_namespace.yml') }}"
  register: result

- name: Print out the result
  debug:
    var: result  

- name: Create Operator Group for Cluster Logging Operator
  k8s:
    state: present
    definition: "{{ lookup('file', 'clo_og.yml') }}"
  register: result

- name: Print out the result
  debug:
    var: result  

- name: Create subscription for Cluster Logging Operator
  k8s:
    state: present
    definition: "{{ lookup('file', 'clo_subscription.yml') }}"
  register: result

- name: Print out the result
  debug:
    var: result  

- name: Pause for installations to start
  pause:
    seconds: "15"
    
- name: Get a list of all CSVs from any namespace
  k8s_info:
    api_version: operators.coreos.com/v1alpha1
    kind: ClusterServiceVersion
    namespace: openshift-logging
  register: csv_list

- name: Print out the result
  debug:
    var: csv_list

- name: Get the Cluster Logging Operator CSV Name
  set_fact:
    clusterloggingCsvName:  "{{ item.metadata.name  }}"
  with_items: "{{ csv_list.resources }}"
  when: '"clusterlogging" in item.metadata.name'

- name: Print out the result
  debug:
    var: clusterloggingCsvName

- name: Wait until CSV is Installed
  ignore_errors: yes
  command: oc get csv {{clusterloggingCsvName|quote}} -o jsonpath --template='{$.status.phase}' -n openshift-logging
  register: csv
  retries: 30
  delay: 10
  until: csv.stdout == "Succeeded"

- name: Print out the result
  debug:
    var: csv

- name: Create ClusterLogging API for Cluster Logging Operator
  k8s:
    state: present
    definition: "{{ lookup('file', 'clusterlogging.yml') }}"
  register: result

- name: Print out the result
  debug:
    var: result

# NOTE:  For next step, make sure to set the following variables:
#        log_forwarding_host - the host name or IP address of the LogStash server
#        log_forwarding_port - the port where LogStash is listening (4000 is the default for FluentD input plugin)

- name: Create Log Forwarding API for Cluster Logging Operator
  k8s:
    state: present
    definition: "{{ lookup('template', 'logforwarding.j2') }}"
  register: result

- name: Print out the result
  debug:
    var: result